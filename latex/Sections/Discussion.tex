\chapter{Discussion}

The results section was once again

\section{Pipeline Evaluation}

Small analyses such as the 3-bit and 5-bit pedigrees examined in the previous chapter are generally not very informative for linkage by themselves. For the 3-bit pedigree we observed multiple low LOD score flat peaks that did not exclude many regions, and for the 5-bit pedigree we saw fewer flat peaks with slightly higher LOD scores and still not that many regions of exclusion.

That is not to say small pedigrees are unusable; weak studies can lend strength to larger analyses where the linkage region is one of a multitude of peaks which can be narrowed down by the extra regions of exclusion given by a smaller analysis. Similarly, large analyses are of great assistance to smaller ones, where we witnessed a 29-bit pedigree reconstitute the parental genotypes of two 3-bit pedigrees in Figure~\pref{fig:res:29bitpedother}.

Filtering errors such as those indicated by GRR, gender, or Mendelian errors are typically not hindered by the size of analysis\footnote{except for GRR runtimes, where window polling increases with the number of total genotypes} and problems are detected at the trio level. Often, small pedigrees are later expanded into larger ones by genotyping the rest of the family in order to improve the LOD score. By catching these small trio errors early on, a researcher can make a more informed decision on whether to pursue the family for further linkage. As we saw in the pedigree in Figure~\pref{fig:res:7bitgenotypes}, parents sometimes have to falsify their children's genealogy for personal and legal reasons, and this can be cause for many a headache for the technician who has to reconcile the true pedigree.

Medium size analyses (7-bit to 18-bit) are usually in the vicinity where such scenarios occur because the linkage peaks are reasonably high ($>$ 1) and few in number ($<$ 6), meaning that there is more room for a researcher to "play" with analysis by running more variations upon the input sets to maximize the analysis. Again, it should be stated that maximizing an analysis by small modifications to the pedigree and penetrance model are not necessarily a true representation of the phenotype, but only what is expected or allowed according to the classical ideals of genetics (e.g. parent-offspring genotype variance and Mendelian inheritance).

For large analyses ($\geq$ 19-bit) fewer analyses are possible due to time constraints, where Table~\pref{table:res:allegrosinglechroms} shows that the timeframe shifts from a under an hour ($\sim$ 2800 seconds $\sim 47$ minutes) to a few days ($\sim$ 945000 seconds $\sim 11$ days). 

Hardware limitations (namely RAM) also make Allegro largely unuseable on a genome-wide scale because the over 19-bit modification switches off the \gls{comp:CUDD} capability which would otherwise minimize the number of repeated calculations employed in the linkage. It is better suited to analysing individual chromosomes, preferably higher numbered chromosomes since the lower chromosomes begin to scale exponentially with runtime as shown in Figure~\pref{fig:res:singleplots}.  Simwalk is of more use on the genome-wide level, but risks not being able to accurately determine X-linked analyses.


\subsection{Pipeline Summary}

The filtering portion of the pipeline seems to work extremely well for all types of pedigrees and penetrances, and is the main strength of the pipeline. The combination of Mendelian errors, GRR relations, gender validation, and unlikely genotypes form easily understandable plots of the quality of the data. Errors are pinpointed to specific families and specific individual(-trios), and the quick runtime of the filtering core ($<$ 5 minutes) means that changes to the pedigree can be tested and evaluated very easily. 

In relation to the rest of the pipeline, analyses are typically brute-force driven in order to maximize the LOD score as permitted by the reasonable runtimes for all pedigrees under the 19-bit limit. Over the limit requires more thought before an analysis is run, often requiring a pedigree to be split into smaller families\footnote{especially for consanguinous pedigrees}  to form a smaller-bit combined analysis that may provide more visual cues on which specific chromosomes to explore when running the larger pedigree, rather than risk the time cost of running a full genome analysis via Simwalk\footnote{Or Allegro, if the pedigree is X-linked and time is not a crucial issue.}.


\section{HaploHTML5 Evaluation}

\subsection{HaploBlock Resolution}
As shown in Figure~\ref{fig:res:haplo29compare} and Figure~\ref{fig:res:haplo23compare} in Section~\prefstar{ref:res:hapcomp} HaploHTML5 resolves autosomal haplotypes just as well as HaploPainter, with the added benefit of comparing haplotypes of any selected individuals in a side-by-side view. 

The 15-bit X-linked pedigree shown in Figures \{~\ref{fig:res:haplo20comparetop},~\ref{fig:res:haplo20comparemid},~\ref{fig:res:haplo20comparebot}\} on pages~\pageref*{fig:res:haplo20comparetop} to ~\pageref*{fig:res:haplo20comparebot} shows how the chromosome X limitation inherent in HaploPainter is overcome in HaploHTML5, where the highly improbably blocks shown in HaploPainter are resolved correctly using the appropriate penetrance model.

\subsection{UI Operability}
The PedCreate view is intuitive and simple, with two types of relations of possible (mate-mate, and parent-offspring) with valid connections being highlighted to the user via red or white anchor points\footnote{See Figure\pref{fig:haplo:pedcreate} as an example}. Individuals are trivially added, modified, deleted, and can be moved around with their various relationship lines updating their positions (and their siblings and/or mates lines) accordingly. The resultant pedigree can be exported to pedfile, with optional positional meta tags which can be reread back into the application either by manually selecting the exported file, or by saving the pedigree into local storage and reloading it upon startup.

The SelectionView mode as shown in Figure\pref{fig:haplo:selectmode} allows for multi-family selection which simplifies the comparison process, especially when comparing the disease locus (found via linkage) within combined analyses. The \gls{comp:DOS} view outlined in page~\pageref{ref:haplo:dos} is in intriguing concept that aims to simplify generational representation upon the selected indivduals under analysis, but ultimately complicates it. It may be better to simply represent the pedigrees of all selected individuals via a small \gls{comp:mipmap} thumbnail in the corner of the screen.

The Homology tools mode is a useful tool for scoring regions of sequence homology under heterozygous, homozygous, and compound heterozygous models. These scores are plotted vertically against the marker scale (see Figure\pref{fig:haplo:homomode}) for quick scrolling, and can also be output to a text file for later inspection. However, the mode can be confusing to interpret due to the non-binary nature in which it scores homology (see page~\pageref{ref:haplo:homologyscoredet}) and the modified HaploPainterRFH script is better in highlighting regions of homology as shown in Figure\pref{fig:res:haplo29comparehomology}. It would be more beneficial for the end-user if the homology tools merely outlined the regions that met that maximum homology score for the scenario (as in the case of HaploPainterRFH) instead of trying to overlay all the data at once.

\subsection{Performance}

Performance testing of the A* best-first algorithm upon a single set of test data gave negligible runtimes on Desktop (0.041 s) and Laptop (0.302 s). Mobile (1.523 s) produced a minor noticeable delay, but still produced the correct haplotypes, which was unexpected given the lightweight nature of its Javascript engine (EmbedLite) not necessarily incorporating all the features of ECMCAScript6 specification. The overall differences seem to be a factor of 10 from Mobile $\rightarrow$ Laptop $\rightarrow$ Desktop, but the differences between OS and browser upon the same hardware appears to be function specific as expected \cite{ratanaworabhan2010jsmeter}, with minor speed increases in Chromium over Firefox.

HaploPainter runs on Perl and relies upon \gls{comp:Tk}, \gls{comp:Cairo}, \gls{comp:Sort Naturally}, and \gls{comp:DBI} Perl dependencies to be present in the system. The more dependencies a program has, the less likely it is to be ported to other platforms successfully since it's dependencies must be ported first. Perl itself also is not very common on \gls{comp:ARM} devices\footnote{The Jolla mobile device used in the Results chapter runs an OS aimed at Linux and shell enthusiasts; Perl did exist upon it, however none of the Perl dependencies had been ported to it and so HaploPainter could not run on it.}. However, the only dependency that HaploHTML5 requires is a web-browser with Javascript capability, immediately making it much more portable than HaploPainter.


\section{Future Work}


\subsection{Pipeline improvements}
Quick retesting usually involves modifying the main pipeline script to some degree, in order to prematurely terminate the analysis after the filtering stage. This is trivial for any technician familiar with Bash or Unix, but may prove more difficult for those used to graphical interfaces. Each run also comes equipped with an automatic readme generator in order to facilitate user logging webUI easier.


\subsection{Effective Filesystem Management}

Why Upgrading Disk Technology Wont Necessarily Lend a Performance Increase...

The working directory should be seen as a temporary file system at best, since many linkage programs generate a substantial amount of temporary files during processing. Though modern file systems are excellent at keeping track of and recovering files (a task known as \gls{comp:journaling}), it is still good practice to "mount a scratch monkey" such that temporary working files are isolated away from archived ones in distinctly separate partitions. 

It is preferable to isolate the two file systems on different disks altogether, such that a disk-wide read error on a given hard disk won't affect the operation of another. Most modern file systems have journaling enabled by default, which writes extra data to disk so that a recovery is possible in the event of a crash, but this becomes more problematic with Solid State Drives (\gls{comp:SSD}s) which have a limited number of read/write operations, and are prone to seizing should an operating system over-breach those limits.

This is not an improbable occurrence; operating systems routinely write temporary files to disk in order to manage RAM constraints in a process known as \gls{comp:paging}, where not immediately required portions of system memory can be sequestered into slower disk storage to free up space in the RAM for more higher priority tasks. 

When a system begins to run low on \gls{comp:RAM} (as in the frequent case of extensive linkage analysis), a great deal of paging is performed to keep the linkage program in memory, leading to a substantial amount of disk writes on a journaled file-system. Mechanical (spin) drives can handle these read/write requests robustly, but they are an order of magnitude slower than SSDs and significant bottlenecks can occur where the system has to wait for the disk to be ready in order to perform a block\footnote{If multiple sequential operations are requested on same contiguous portion of a disk, then the operating system groups the individual requests under a large contiguous 'block' request, that performs one long read/write operation instead of several short ones.}    read/write to it.

It is clear that SSDs offer no advantage at all in the use-case of frequent large temporary file activity, since though the operating system will rarely have to wait in order to perform a read/write, the limitations on the number of these operations shortens their lifetime considerably.

The best long-term compromise is to use mechanical disks with a filesystem without journaling; either an older filesystem where journaling was never implemented (though file size constraints may be enacted), or a newer filesystem where journaling is disabled upon initialization. This reduces the total number of read/write operations whilst still ensuring disk longevity.

Recoverability may be jeopardized, but the temporary nature of the files within the system imply that a re-run of the same analysis would not be resumable in any case; for it is the operating system that dispatches the temporary files to the linkage programs, and not the linkage program itself (with the  notable exception of \gls{prog:Simwalk}).


\subsubsection{RAID Configuration}

One extra caveat to reduce the number of operations upon a single disk \textit{and} keep some level of redundancy is to distribute the load across several disk drives in a standard redundant array of independent disks (\gls{comp:RAID}); specifically a RAID-5 level setup. RAID relies on the method of storing contiguous data across several mediums (a concept known as \gls{comp:striping}), as well as ensuring the data is correct and error-checkable without having to scan the entire block of data (a concept known as \gls{comp:parity}). The failure of one disk drive would not be enough to impede the setup and an ongoing analysis could be reconstituted from the other remaining disks, though a minimum of three identically sized disks is required.

These features lend well to a large temporary filesystem, since the storage capacities of multiple disks can now be combined to store a great amount of data, as well as the number of operations on any one disk being reduced by a factor of \textit{n} (for \textit{n} disks in the setup). The setup will still have the same read/write speeds as an ordinary disk, but there will not be any added complexity since the operating system will detect it as a single volume.


The pipeline

pipeline web interface

concrete.js, anotatations, NodeJS implements



Scores between Allegro and GeneHunter tend to differ by only $\pm$ 0.2. Any larger discrepancies are suspect, and usually prompt a Simwalk run as a precautionary third control.




\subsection{HaploHTML5 Improvements}

\subsubsection{Founder Colour Group Representation}
One current problem with the application is the limited colour space for the founder allele groups to be allocated to. Under the \gls{comp:HSV} model with a hue scored from 0\% to 100\% differences in colour become harder to distinguish below the 15\% range as shown in Figure~\ref{fig:disc:huescale}. 

For $f$ founders, there are $2f$ unique colours that must be assigned to their respective alleles, meaning that as soon as the pedigree begins to have more than 3 founders\footnote{$ \frac{100}{15} \sim 7 $ unique colour groups for all founder alleles $ \sim 3$ founders.}, the colour groups become more ambiguous.

One solution would be to change the the Value component of the HSV model to take on darker and/or lighter components, which would double and/or triple the number of available colours, but would require changing the colour of the genotypes text which reside on top of the coloured block. 

\fignow{Sections/images/hue_scale.png}
{Hue Scale (\%). Distinct colours are more visible at 15\% increments.}
{fig:disc:huescale}
{0.7}{}

It is possible to move the genotype text out next to the block as represented in HaploPainter, but this would reduce the amount of available horizontal space since more width would have to allocated per individual.

Another solution would be to regenerate the founder allele colour groups to only those currently under selection, but introduces the issue of inconsistent colouring across multiple runs on the same data.


\subsubsection{Annotation and Sharing}

The application currently operates in-browser via either local or web deployment, and analyses are restricted to the 


\subsubsection{Framework overhaul}
Concrete.js \cite{concretejs}


